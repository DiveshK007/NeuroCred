# Incident Response Runbook

## Incident Severity Levels

### P0 - Critical
- Complete service outage
- Data loss or corruption
- Security breach

### P1 - High
- Partial service degradation
- Performance issues affecting users
- Database connectivity issues

### P2 - Medium
- Non-critical feature failures
- Minor performance degradation
- Non-user-facing issues

### P3 - Low
- Cosmetic issues
- Documentation errors
- Enhancement requests

## Incident Response Process

### 1. Detection

**Sources:**
- Monitoring alerts (Sentry, Prometheus)
- User reports
- Health check failures
- Automated tests

### 2. Triage

**Immediate Actions:**
1. Acknowledge incident
2. Assess severity
3. Notify team
4. Create incident ticket

### 3. Investigation

**Steps:**
1. Check service status
2. Review recent changes
3. Check logs and metrics
4. Identify root cause

### 4. Resolution

**Actions:**
1. Implement fix
2. Verify resolution
3. Monitor for stability
4. Document incident

### 5. Post-Incident

**Tasks:**
1. Write post-mortem
2. Update runbooks
3. Implement preventive measures
4. Review monitoring/alerting

## Common Incident Scenarios

### Scenario 1: Complete API Outage

**Symptoms:**
- All API endpoints returning 500
- Health checks failing
- No successful requests

**Response:**
1. Check pod status: `kubectl get pods -n neurocred-prod`
2. Check logs: `kubectl logs -f deployment/backend -n neurocred-prod`
3. Check database connectivity
4. Check Redis connectivity
5. Restart services if needed
6. Rollback if recent deployment

**Commands:**
```bash
# Check status
kubectl get all -n neurocred-prod

# Restart backend
kubectl rollout restart deployment/backend -n neurocred-prod

# Rollback if needed
kubectl rollout undo deployment/backend -n neurocred-prod
```

### Scenario 2: Database Connection Failures

**Symptoms:**
- Database connection errors in logs
- "Too many connections" errors
- Query timeouts

**Response:**
1. Check database pod status
2. Check connection pool metrics
3. Verify database credentials
4. Check network policies
5. Increase connection pool if needed

**Commands:**
```bash
# Check database
kubectl get pods -l app=postgres -n neurocred-prod

# Check connections
kubectl exec -it postgres-pod -- psql -U neurocred -c "
SELECT count(*) FROM pg_stat_activity;
"
```

### Scenario 3: High Error Rate

**Symptoms:**
- Increased error rate in Sentry
- 500 errors increasing
- User complaints

**Response:**
1. Check Sentry for error patterns
2. Review recent deployments
3. Check resource usage
4. Review application logs
5. Check external dependencies

**Commands:**
```bash
# Check error rate
curl http://localhost:8000/metrics | grep http_requests_total

# Check Sentry
# Review Sentry dashboard for error trends
```

### Scenario 4: Performance Degradation

**Symptoms:**
- Slow API responses
- High latency
- Timeout errors

**Response:**
1. Check resource usage (CPU/Memory)
2. Check database query performance
3. Check Redis performance
4. Review slow queries
5. Scale services if needed

**Commands:**
```bash
# Check resource usage
kubectl top pods -n neurocred-prod

# Scale services
kubectl scale deployment/backend --replicas=5 -n neurocred-prod
```

## Communication

### Internal Communication

- **Slack Channel**: #neurocred-incidents
- **Status Page**: Update during incidents
- **Email**: Notify team for P0/P1 incidents

### External Communication

- **Status Page**: Public status updates
- **Twitter**: For major incidents (if applicable)
- **Email**: Notify users for P0 incidents

## Escalation

### Escalation Path

1. **On-Call Engineer**: Initial response
2. **Team Lead**: For P1+ incidents
3. **Engineering Manager**: For P0 incidents
4. **CTO**: For critical security breaches

### Escalation Criteria

- P0: Escalate immediately
- P1: Escalate if not resolved in 1 hour
- P2: Escalate if not resolved in 4 hours
- P3: Escalate if not resolved in 24 hours

## Post-Incident Review

### Post-Mortem Template

1. **Incident Summary**
   - What happened
   - When it happened
   - Impact

2. **Timeline**
   - Detection time
   - Resolution time
   - Key events

3. **Root Cause**
   - Primary cause
   - Contributing factors

4. **Resolution**
   - Steps taken
   - What worked
   - What didn't work

5. **Prevention**
   - Action items
   - Monitoring improvements
   - Process changes

### Action Items

- [ ] Fix root cause
- [ ] Update monitoring
- [ ] Update runbooks
- [ ] Improve alerting
- [ ] Update documentation

